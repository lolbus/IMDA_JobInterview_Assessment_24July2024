{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1eb9df-0cd1-4bff-9ad3-c21964af42ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.6455]), Std: tensor([0.4215])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from SimpleCNN import SimpleCNNArchitecture\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "# Directories\n",
    "image_dir = r'C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\sampleCaptchas\\letters\\input'\n",
    "label_dir = r'C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\sampleCaptchas\\letters\\output'\n",
    "\n",
    "# Custom Dataset\n",
    "class CharacterDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = glob.glob(os.path.join(image_dir, '*.jpg'))\n",
    "        self.label_files = glob.glob(os.path.join(label_dir, '*.txt'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        label_path = os.path.join(self.label_dir, os.path.basename(img_path.replace(\"in\", \"out\")).replace('.jpg', '.txt'))\n",
    "        \n",
    "        image = Image.open(img_path).convert('L')  # Convert image to grayscale\n",
    "        with open(label_path, 'r') as f:\n",
    "            label = f.read().strip()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = ord(label) - ord('A') if 'A' <= label <= 'Z' else ord(label) - ord('0') + 26\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Function to compute mean and std\n",
    "def compute_mean_std(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_samples += batch_samples\n",
    "\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# Create a dataset without normalization to compute mean and std\n",
    "dataset = CharacterDataset(image_dir, label_dir, transform=transforms.ToTensor())\n",
    "mean, std = compute_mean_std(dataset)\n",
    "print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "dataset = CharacterDataset(image_dir, label_dir, transform=transform)\n",
    "\n",
    "# Perform train/validation/test split\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, loss function, optimizer\n",
    "model = SimpleCNNArchitecture()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learn_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "\n",
    "# Lambda function for the scheduler\n",
    "def lr_lambda(epoch):\n",
    "    return max(1 - epoch / 300, 0.01)\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    best_val_acc = 0.0\n",
    "    best_model = copy.deepcopy(model)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss \n",
    "        val_accuracy = correct / total\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_val_acc = val_accuracy\n",
    "            print(\"!!!NEW BEST VAL ACC\", best_val_acc)\n",
    "            print()\n",
    "        print(f'Current Best Val Acc: {best_val_acc:.4f} Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    best_model.eval()\n",
    "    return best_model\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    # Assuming 'model' is your trained model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8d0b7-086e-40ab-bb0a-7a83e14c962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "best_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=300)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(best_model, test_loader)\n",
    "\n",
    "# Save the model\n",
    "best_model.eval()\n",
    "torch.save(best_model.state_dict(), 'imda_technical_test_pytorch_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88af1de-a452-47e6-a54b-ab9a00ca64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'InferredCharacters': '5I8VE'}\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD//gA+Q1JFQVRPUjogZ2QtanBlZyB2MS4wICh1c2luZyBJSkcgSlBFRyB2NjIpLCBkZWZhdWx0IHF1YWxpdHkK/9sAQwAIBgYHBgUIBwcHCQkICgwUDQwLCwwZEhMPFB0aHx4dGhwcICQuJyAiLCMcHCg3KSwwMTQ0NB8nOT04MjwuMzQy/9sAQwEJCQkMCwwYDQ0YMiEcITIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIy/8AAEQgAHgA8AwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/aAAwDAQACEQMRAD8A9xma8hQN50By6r/qT3YD+971G9zNFcNFLd2seEVgXjIzkkf3/b9a5r4j+JLvw3pukPbT2dv9t1a3s5ri7UskEbbmLnDL02A8kDGfqE8N6nd6pfaiv9uWl1qEEHlFH0ma0e1YgMplikk3sp3Aj7ucNg+gBvTXgPlo11byqWDnygo+6wPUvgZ/xqRdVLQeaq2pOwv5YuPmwBnGNvWuW8IeJ9d1Qa9NqRsrhNM1G506O3sbN0lnkiCkMC8xUbhkbTgA4+bFY2kePtR1zVtJtJ0srs3Uk/2zS7GGZbvSwuVBmZzghc7XyqEsRsD9CAekW1pbSxu8lvE7GWTLMgJPzmknitbSa2lEcMI80guFC8bG715mvxG1qD4frdx21k3idtVbSmsDBL5YuzMf3e7fgnZzndjPftU+l+ONR1rVdTtpdf8ADmn3NjrE1ja2U9ozT3AB2KwVrhDzuIwB1B+lAHoEdzE6sz6mEO9wFDR4ADEDqPTFPt7sND8zySEMw3rCzBgGIByox0x0pm6VLG3gktpFKNCpYlSMhl9Dn9K1qAOY1fT9TvLBpNBaysL2CffFJLGrxylGIKSDZkI2MEqQw4IPGDh6R4f1i38X3fiC8j01dQXTzp1taW0r+UY1KybpJTHkuzNjhMADoxrurP8A1Lf9dZP/AENqzZbyK01GSRlcglh8oGfuxf4UAch4a8N65p9tr0GpiC3t9X1K8vPtGmXcrz27Srs2qPJH3cEh89cfLVfSfCOpKPDFjdfYLWx8OyvJFeabDMtzdEZUKVKARK4JMoDPvPpnI9F0+QSWm9QQGkc89eXNVYrdbpTN5dr88jqN9vuOQxGSd3PSgDjbT4du3xH/AOEie4f+xi73w01nIX7fkoJfKK7fuYbdndv5zim6Hpnijw7LrqW+l6Zd2uo67NfxzSai8RUPIoClRC3Py9c9/au3itYvtMSSQWrKyM/ywBSCCvufWrnkw+X5HlJ5f9zaNvr0oAS8/wBSv/XWP/0Nas1mX0NraW/nrbxqY3Q5RAD94dKj/wCEgtP+ec/5D/GgD//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=60x30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample usage of trained torch model\n",
    "from TorchInferenceEngine import load_torch_ai_model\n",
    "from PIL import Image\n",
    "\n",
    "torch_ai_model = load_torch_ai_model(model_path = 'imda_technical_test_pytorch_model.pth')\n",
    "sample_catpcha_image_path = r'C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\sampleCaptchas\\input\\input19.jpg'\n",
    "\n",
    "ir = torch_ai_model.get_image_information(sample_catpcha_image_path)\n",
    "print(ir)\n",
    "\n",
    "sample_image = Image.open(sample_catpcha_image_path)\n",
    "display(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a92a6-80cd-42e4-9983-3a7bb6515fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7edf5-1e5d-4cda-9bb7-ef70b1546b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

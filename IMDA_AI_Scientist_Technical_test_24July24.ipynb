{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a3157995-4be5-440f-9227-3506caeab65f",
   "metadata": {},
   "source": [
    "(B) Deliverable  \r\n",
    "1 \tREADME.md \r",
    "2\tPython code (use the following template and feel free to add the necessary methods)\r\n",
    "class Captcha(object):\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def __call__(self, im_path, save_path):\r\n",
    "        \"\"\"\r\n",
    "        Algo for inference\r\n",
    "        args:\r\n",
    "            im_path: .jpg image path to load and to infer\r\n",
    "            save_path: output file path to save the one-line outcome\r\n",
    "        \"\"\"\r\n",
    "        pass\r\n",
    "We would like to learn more about the way you frame the problem and formulate the solution. You may check in the deliverables to your Github or Gitlab and share with us the link.\r\n",
    "b and share with us the link.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8ecdb-a25f-4ff9-99e8-5fe866aa20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "\n",
    "class dataloader(object):\n",
    "    def __init__(self, input_dir, output_dir, training_size = 99):\n",
    "    # Initialize an empty dictionary to store the data\n",
    "        data_dict = {}\n",
    "        data_loaded = 0\n",
    "        # Loop through all files in the input directory\n",
    "        for filename in os.listdir(input_dir):\n",
    "            \n",
    "            if filename.endswith(\".jpg\"):\n",
    "                # Get the full path of the image file\n",
    "                img_path = os.path.join(input_dir, filename)\n",
    "                \n",
    "                # Construct the corresponding txt file name\n",
    "                txt_filename = filename.replace(\".jpg\", \".txt\")\n",
    "                txt_filename = txt_filename.replace(\"input\", \"output\")\n",
    "                txt_path = os.path.join(output_dir, txt_filename)\n",
    "                \n",
    "                if os.path.exists(txt_path):\n",
    "                    with open(txt_path, 'r') as txt_file:\n",
    "                        label = txt_file.read().strip()             \n",
    "                        # Store the image path and label in the dictionary\n",
    "                        data_dict[img_path] = label\n",
    "                        data_loaded += 1\n",
    "                        print(f\"Pairing {filename} with {txt_filename}\")\n",
    "                        if data_loaded == training_size:\n",
    "                            break\n",
    "\n",
    "    \n",
    "        self.data_dict = data_dict\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc55aad-9067-49b1-ba00-3455bfe5eb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d08688-5348-449d-89fa-57a2611995df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import langchain\n",
    "from langchain.chains import TransformChain\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import chain\n",
    "import base64\n",
    "import json\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "openai_api_key = \"sk-proj-EdTJfI06sx021wOqwqMUT3BlbkFJQqpltq3qDR6x4yVxElu2\"\n",
    "\n",
    "class llm_model(object):\n",
    "    def __init__(self, vision_prompt, few_shot_prompt, finetune = True, training_data = None):\n",
    "        self.llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.0, model=\"gpt-4o\", max_tokens=1024)\n",
    "        self.format_instruction = format_instruction = \"\"\"\n",
    "             Provide your output in the format of a python dict object code {\"InferredCharacters\": \"XXXXX\"}.\n",
    "             Do not provide any other information. \n",
    "             Do not provide the backticks or any characters not related to the dict string such that I can convert to dict object directly by json.load(result)\"\"\"\n",
    "        self.vision_prompt = vision_prompt \n",
    "        if finetune:\n",
    "            self.vision_prompt = self.vision_prompt + few_shot_prompt\n",
    "            for data_path in training_data.data_dict.keys():\n",
    "                encoded_image_data = encode_image(data_path)\n",
    "                fine_tune_data = f\"\"\"```Encoded image data: {encoded_image_data} -> True label: {training_data.data_dict[data_path]}``` \\n\"\"\"\n",
    "                self.vision_prompt = self.vision_prompt + fine_tune_data\n",
    "\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "            \n",
    "def load_image(inputs: dict) -> dict:\n",
    "    \"\"\"Load image from file and encode it as base64.\"\"\"\n",
    "    image_path = inputs[\"image_path\"]\n",
    "    image_base64 = encode_image(image_path)\n",
    "    return {\"image\": image_base64}\n",
    "\n",
    "load_image_chain = TransformChain(\n",
    "    input_variables=[\"image_path\"],\n",
    "    output_variables=[\"image\"],\n",
    "    transform=load_image\n",
    ")\n",
    "\n",
    "\n",
    "finetune_input_dir = r\"C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\fine-tune-data\\input\"\n",
    "finetune_output_dir = r\"C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\fine-tune-data\\output\"\n",
    "training_data = dataloader(input_dir = finetune_input_dir, output_dir = finetune_output_dir)\n",
    "llm_agent = llm_model(vision_prompt, few_shot_prompt, finetune = True, training_data = training_data)\n",
    "\n",
    "@chain\n",
    "def image_model(inputs: dict) -> str:\n",
    "    \"\"\"Invoke model with image and prompt.\"\"\"\n",
    "    model = llm_agent.llm\n",
    "    # print(\"Final vision prompt\\n\", inputs[\"prompt\"])\n",
    "    msg = model.invoke(\n",
    "             [SystemMessage(content=llm_agent.format_instruction),\n",
    "              HumanMessage(\n",
    "             content=[\n",
    "             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n",
    "             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n",
    "             ])]\n",
    "             )\n",
    "    return msg.content\n",
    "\n",
    "@tool\n",
    "def convert_output_to_dict(llm_output:str) -> dict:\n",
    "    \"\"\"Convert the output of the LLM to a simple dict with only the predicted output as the key\"\"\"\n",
    "    try:\n",
    "        response_dict = json.loads(llm_output)\n",
    "    except json.JSONDecodeError:\n",
    "        response_dict = {\"error\": \"Invalid JSON response\"}\n",
    "    return response_dict\n",
    "    \n",
    "\n",
    "\n",
    "def get_image_information(image_path: str, llm_model) -> dict:\n",
    "    vision_prompt = llm_model.vision_prompt\n",
    "    vision_chain = load_image_chain | image_model | convert_output_to_dict\n",
    "    return vision_chain.invoke({'image_path': f'{image_path}', \n",
    "                               'prompt': vision_prompt})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fe629-60ea-4e3e-aa90-1f21071aa464",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import langchain\n",
    "from langchain.chains import TransformChain\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import chain\n",
    "import base64\n",
    "import json\n",
    "from langchain.tools import tool\n",
    "from InferenceDataloader import dataloader\n",
    "\n",
    "\n",
    "# Prompt Engineering portion\n",
    "\n",
    "vision_prompt = \"\"\"Given the Captcha image, where \n",
    "They resemble each other very much where the identify that the texture, nature of the font, spacing of the font, morphological characteristic of the letters and numerals arev very consistent.\n",
    "Infer the image's characters (Only capital letters and numbers)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\"\"Few shot prompting fine tunning: it is very important to take note of the difference between:\n",
    "0 and O  \n",
    "\n",
    "and\n",
    "\n",
    "1 and I\n",
    "\n",
    "In our dataset, their difference are consistent ('O' being more circular than '0' and having more rounding pixels at the top and bottom tip of the character and 'I' having a longer flat surface at the tip of the character than '1') and you must fine-tune yourself to adapt to the font format.\n",
    "Pay attention to the top tip of the character when trying to distinguish these four characters.\n",
    "A list of sample data describing the difference between '0', 'O', '1' and 'I' are provided to allow you to fine-tune your performance for the given task: \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class llm_model(object):\n",
    "    def __init__(self, vision_prompt, few_shot_prompt, finetune = True, training_data = None):\n",
    "        self.llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.0, model=\"gpt-4o\", max_tokens=1024)\n",
    "        self.format_instruction = format_instruction = \"\"\"\n",
    "             Provide your output in the format of a python dict object code {\"InferredCharacters\": \"XXXXX\"}.\n",
    "             Do not provide any other information. \n",
    "             Do not provide the backticks or any characters not related to the dict string such that I can convert to dict object directly by json.load(result)\"\"\"\n",
    "        self.vision_prompt = vision_prompt \n",
    "        if finetune:\n",
    "            self.vision_prompt = self.vision_prompt + few_shot_prompt\n",
    "            for data_path in training_data.data_dict.keys():\n",
    "                encoded_image_data = encode_image(data_path)\n",
    "                fine_tune_data = f\"\"\"```Encoded image data: {encoded_image_data} -> True label: {training_data.data_dict[data_path]}``` \\n\"\"\"\n",
    "                self.vision_prompt = self.vision_prompt + fine_tune_data\n",
    "\n",
    "def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "            \n",
    "def load_image(inputs: dict) -> dict:\n",
    "    \"\"\"Load image from file and encode it as base64.\"\"\"\n",
    "    image_path = inputs[\"image_path\"]\n",
    "    image_base64 = encode_image(image_path)\n",
    "    return {\"image\": image_base64}\n",
    "\n",
    "load_image_chain = TransformChain(\n",
    "    input_variables=[\"image_path\"],\n",
    "    output_variables=[\"image\"],\n",
    "    transform=load_image\n",
    ")\n",
    "\n",
    "finetune_input_dir = r\"C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\fine-tune-data\\input\"\n",
    "finetune_output_dir = r\"C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\fine-tune-data\\output\"\n",
    "training_data = dataloader(input_dir = finetune_input_dir, output_dir = finetune_output_dir)\n",
    "openai_api_key = \"sk-proj-EdTJfI06sx021wOqwqMUT3BlbkFJQqpltq3qDR6x4yVxElu2\"\n",
    "llm_agent = llm_model(vision_prompt, few_shot_prompt, finetune = True, training_data = training_data)\n",
    "\n",
    "\n",
    "\n",
    "@chain\n",
    "def image_model(inputs: dict) -> str:\n",
    "    \"\"\"Invoke model with image and prompt.\"\"\"\n",
    "    model = llm_agent.llm\n",
    "    # print(\"Final vision prompt\\n\", inputs[\"prompt\"])\n",
    "    msg = model.invoke(\n",
    "             [SystemMessage(content=llm_agent.format_instruction),\n",
    "              HumanMessage(\n",
    "             content=[\n",
    "             {\"type\": \"text\", \"text\": inputs[\"prompt\"]},\n",
    "             {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{inputs['image']}\"}},\n",
    "             ])]\n",
    "             )\n",
    "    return msg.content\n",
    "\n",
    "@tool\n",
    "def convert_output_to_dict(llm_output:str) -> dict:\n",
    "    \"\"\"Convert the output of the LLM to a simple dict with only the predicted output as the key\"\"\"\n",
    "    try:\n",
    "        response_dict = json.loads(llm_output)\n",
    "    except json.JSONDecodeError:\n",
    "        response_dict = {\"error\": \"Invalid JSON response\"}\n",
    "    return response_dict\n",
    "\n",
    "\n",
    "#OpenAI Inference Engine\n",
    "class load_OpenAI_ai_model(object):\n",
    "    def __init__(self):\n",
    "        self.llm_agent = llm_agent\n",
    "\n",
    "    def get_image_information(self, image_path: str, llm_model) -> dict:\n",
    "        vision_prompt = self.llm_agent.vision_prompt\n",
    "        vision_chain = load_image_chain | image_model | convert_output_to_dict\n",
    "        return vision_chain.invoke({'image_path': f'{image_path}', \n",
    "                                   'prompt': vision_prompt})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6136ecb-65e6-4467-8a2c-901db5ff33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage of OpenAI trained torch model\n",
    "import OpenAIInferenceEngine\n",
    "from OpenAIInferenceEngine import load_OpenAI_ai_model, llm_model\n",
    "from InferenceDataloader import dataloader\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "openai_api_key = \"sk-proj-OP2IckZNWS1a9GsDQLYzT3BlbkFJkQ3fVVD3BpJw4cG8b6Yv\"\n",
    "\n",
    "\n",
    "# Use data used for zero-shot prompting to fine-tune LLM\n",
    "finetune_input_dir = r\"C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\fine-tune-data\\input\"\n",
    "finetune_output_dir = r\"C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\fine-tune-data\\output\"\n",
    "training_data = dataloader(input_dir = finetune_input_dir, output_dir = finetune_output_dir)\n",
    "OpenAIInferenceEngine.llm_agent = llm_model(finetune = False, training_data = training_data, openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "openai_ai_model = load_OpenAI_ai_model()\n",
    "sample_catpcha_image_path = r'C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\sampleCaptchas\\input\\input19.jpg'\n",
    "\n",
    "ir = openai_ai_model.get_image_information(sample_catpcha_image_path)\n",
    "print(ir)\n",
    "\n",
    "sample_image = Image.open(sample_catpcha_image_path)\n",
    "display(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5accc6b-2eee-4efb-878d-b8a2e6bed2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample usage of torch trained torch model\n",
    "from TorchInferenceEngine import load_torch_ai_model\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "torch_ai_model = load_torch_ai_model(model_path = 'imda_technical_test_pytorch_model.pth')\n",
    "sample_catpcha_image_path = r'C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\sampleCaptchas\\input\\input19.jpg'\n",
    "\n",
    "ir = torch_ai_model.get_image_information(sample_catpcha_image_path)\n",
    "print(ir)\n",
    "\n",
    "sample_image = Image.open(sample_catpcha_image_path)\n",
    "display(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62776d59-5537-4322-acc2-ec9b507fe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Available Strategies:\n",
    "1. OpenAI GPT4o AI Assistant - LLMAIAssistant\n",
    "2. PyTorch self trained model - PytorchCNN\"\"\"\n",
    "\n",
    "class Captcha(object):\n",
    "    def __init__(self, strategy = \"\"):\n",
    "        if strategy == \"LLMAIAssistant\":\n",
    "            import OpenAIInferenceEngine\n",
    "            from OpenAIInferenceEngine import load_OpenAI_ai_model, llm_model\n",
    "            from InferenceDataloader import dataloader\n",
    "            self.openai_ai_model = load_OpenAI_ai_model()\n",
    "            self.caller = openai_ai_model.get_image_information\n",
    "        \n",
    "        elif strategy == \"PytorchCNN\":\n",
    "            from TorchInferenceEngine import load_torch_ai_model\n",
    "            self.torch_ai_model = load_torch_ai_model(model_path = 'imda_technical_test_pytorch_model.pth')\n",
    "            self.caller = self.torch_ai_model.get_image_information\n",
    "        else:\n",
    "            print(f\"{strategy} is not a valid method\")\n",
    "\n",
    "    def __call__(self, im_path, save_path):\n",
    "        \"\"\"\n",
    "        Algo for inference\n",
    "        args:\n",
    "            im_path: .jpg image path to load and to infer\n",
    "            save_path: output file path to save the one-line outcome\n",
    "        \"\"\"\n",
    "        result = self.caller(im_path)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6c0e5a-f0d7-4810-852b-455705844c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'InferredCharacters': 'EGGK4'}\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD//gA+Q1JFQVRPUjogZ2QtanBlZyB2MS4wICh1c2luZyBJSkcgSlBFRyB2NjIpLCBkZWZhdWx0IHF1YWxpdHkK/9sAQwAIBgYHBgUIBwcHCQkICgwUDQwLCwwZEhMPFB0aHx4dGhwcICQuJyAiLCMcHCg3KSwwMTQ0NB8nOT04MjwuMzQy/9sAQwEJCQkMCwwYDQ0YMiEcITIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIy/8AAEQgAHgA8AwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/aAAwDAQACEQMRAD8A9xma8hQN50By6r/qT3YD+971G9zNFcNFLd2seEVgXjIzkkf3/b9a5r4j+JLvw3pukPbT2dv9t1a3s5ri7UskEbbmLnDL02A8kDGfqM648WnQNA1PxPc61p3iOC1h8tDpcQjAfeihGbzXHJkU+oGThsgUAddNeA+WjXVvKpYOfKCj7rA9S+Bn/GpF1UtB5qrak7C/li4+bAGcY29axQ/je0hja4uPD9xJJbSLsjt7iIRXOzMePmcyR7gwPCHHI/u1hweM9S1+bSNHg0uWPWYJkPiCKe1lWKyj2sHwwJyz5zFgsCOTgUAd5bWltLG7yW8TsZZMsyAk/OaSeK1tJraURwwjzSC4ULxsbvXmfhP4i6z4o0TTItKhs5NYuHkl1B1tZXt9Oi818F8PlncL8qAgnkkgDm38QPiLd+HbGc6NBDqN/p+ya8LwssVmjt5a+b84O9i3yoDnGWPA5AO8juYnVmfUwh3uAoaPAAYgdR6Yp9vdhofmeSQhmG9YWYMAxAOVGOmOlM3SpY28EltIpRoVLEqRkMvoc/pWtQBwni7w3f8AiCPRpdJtdPjk0zWIr4+fK0YkEJcFcrG2MnHaquq+G9U8VeH9R8ParbadYedEI45rG5eUllKOGbdEucELwOoLDK9a7yz/ANS3/XWT/wBDas2W8itNRkkZXIJYfKBn7sX+FAHKLoOt6n4z8N+JdYs9KtpNJjuIZ5NPklne5ZkKAYMSlEVt5ALNjdjuTWloOkzaf458Va5K4e11Y2ggSOKQuphiKNuBXA5PGCfwrp9PkElpvUEBpHPPXlzVWK3W6UzeXa/PI6jfb7jkMRkndz0oA4Lwf8P9X8J+HdKudKms4tdgLxahCzOttqEJmdlDnbuWRVb5JNpI+6QVPDvF3w8t9V8Maza6NDPa3+pXpmaSS/mFq8rXC7meIOVJIH9w9B6DHfRWsX2mJJILVlZGf5YApBBX3PrVzyYfL8jyk8v+5tG316UAV3hW1sYbdDIUjaJFMkjOxAZRyzElj7kknvWhWZfQ2tpb+etvGpjdDlEAP3h0qP8A4SC0/wCec/5D/GgD/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=60x30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "captcha_inferencer = Captcha(strategy = \"PytorchCNN\")\n",
    "# declare inferencer and sample inferencing\n",
    "sample_image = \"C:/Users/guang/OneDrive/Desktop/IMDA/sampleCaptchas/input/input00.jpg\"\n",
    "result = captcha_inferencer(sample_image, None)\n",
    "print(result)\n",
    "displayable_image = Image.open(sample_image)\n",
    "display(displayable_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f9715e9-0ce7-4fdd-be9b-69d1a17eb4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m input_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mguang\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIMDA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msampleCaptchas\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mguang\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIMDA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msampleCaptchas\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m inference_data \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader\u001b[49m(input_dir, output_dir)\n\u001b[0;32m      6\u001b[0m corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m attempts \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "input_dir = r\"C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\sampleCaptchas\\input\"\n",
    "output_dir = r\"C:\\Users\\guang\\OneDrive\\Desktop\\IMDA\\sampleCaptchas\\output\"\n",
    "\n",
    "inference_data = dataloader(input_dir, output_dir)\n",
    "\n",
    "corrects = 0\n",
    "attempts = 0\n",
    "for data_path in inference_data.data_dict:\n",
    "    true_label = inference_data.data_dict[data_path]\n",
    "    inferred_output = captcha_inferencer(data_path, None)\n",
    "    predict = inferred_output['InferredCharacters']\n",
    "    print(f\"Predict result: {predict} versus True Label: {true_label}\")\n",
    "    if predict == true_label:\n",
    "        corrects += 1\n",
    "    else:\n",
    "        print(\"incorrect! Predicting\", data_path)\n",
    "    attempts += 1\n",
    "\n",
    "print(f\"Final remarks: corrects {corrects} out of {attempts} attempts. accuracy of {corrects/attempts:.2f}\")\n",
    "\n",
    "\n",
    "# Common errors, mistaken O as 0, I mistake as 1, mistake 0 as O, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7d06d-91d8-4b4e-af98-493567af64aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f14a5a-6314-4f6b-8cab-777d2b655479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

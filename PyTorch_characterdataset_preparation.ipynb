{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076bcbb6-9576-43a6-814c-421d2e657835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing task on .\\torch_training_data\\input\\input00.jpg\n",
      "performing task on .\\torch_training_data\\input\\input01.jpg\n",
      "performing task on .\\torch_training_data\\input\\input02.jpg\n",
      "performing task on .\\torch_training_data\\input\\input03.jpg\n",
      "performing task on .\\torch_training_data\\input\\input04.jpg\n",
      "performing task on .\\torch_training_data\\input\\input05.jpg\n",
      "performing task on .\\torch_training_data\\input\\input06.jpg\n",
      "performing task on .\\torch_training_data\\input\\input07.jpg\n",
      "performing task on .\\torch_training_data\\input\\input08.jpg\n",
      "performing task on .\\torch_training_data\\input\\input09.jpg\n",
      "performing task on .\\torch_training_data\\input\\input10.jpg\n",
      "performing task on .\\torch_training_data\\input\\input11.jpg\n",
      "performing task on .\\torch_training_data\\input\\input12.jpg\n",
      "performing task on .\\torch_training_data\\input\\input13.jpg\n",
      "performing task on .\\torch_training_data\\input\\input14.jpg\n",
      "performing task on .\\torch_training_data\\input\\input15.jpg\n",
      "performing task on .\\torch_training_data\\input\\input16.jpg\n",
      "performing task on .\\torch_training_data\\input\\input17.jpg\n",
      "performing task on .\\torch_training_data\\input\\input18.jpg\n",
      "performing task on .\\torch_training_data\\input\\input19.jpg\n",
      "performing task on .\\torch_training_data\\input\\input20.jpg\n",
      "performing task on .\\torch_training_data\\input\\input21.jpg\n",
      "performing task on .\\torch_training_data\\input\\input22.jpg\n",
      "performing task on .\\torch_training_data\\input\\input23.jpg\n",
      "performing task on .\\torch_training_data\\input\\input24.jpg\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# This code block prepares dataset to train a SimpleCNN to do character inference \n",
    "import os\n",
    "from PIL import Image\n",
    "import PIL\n",
    "from TorchInferenceEngine import crop_image_and_return_5char, coordinates\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Paths to the input and output folders\n",
    "input_folder = r'.\\torch_training_data\\input'\n",
    "output_folder = r'.\\torch_training_data\\output'\n",
    "letters_input_folder = r'.\\torch_training_data\\letters\\input'\n",
    "letters_output_folder = r'.\\torch_training_data\\letters\\output'\n",
    "filter_for_characters = [] # Add characters to this list to prepare customize dataset that only contains the filter characters\n",
    "# leave the above as empty list if you want to prepare a character dataset that includes all characters\n",
    "\n",
    "# Create the output folder if it does not exist\n",
    "os.makedirs(letters_input_folder, exist_ok=True)\n",
    "os.makedirs(letters_output_folder, exist_ok=True)\n",
    "\n",
    "def convert_image_to_white_or_black_r_image(image):\n",
    "    \"\"\"\n",
    "    This function convert a PIL image to a black (0 value) or white (255 value) image\n",
    "    args:\n",
    "        img : PIL image\n",
    "    output:\n",
    "        result img: PIL image\n",
    "    \"\"\"\n",
    "    image_np = np.array(image)\n",
    "    image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).float()\n",
    "    binary_tensor = (image_tensor >= 64).to(torch.uint8)\n",
    "    result_tensor = binary_tensor * 255\n",
    "    result_np = result_tensor.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    result_image = Image.fromarray(result_np)\n",
    "    return result_image\n",
    "\n",
    "\n",
    "# Loop through all the images in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.jpg'):\n",
    "        \n",
    "        # Construct the corresponding txt file name\n",
    "        txt_filename = filename.replace(\".jpg\", \".txt\")\n",
    "        txt_filename = txt_filename.replace(\"input\", \"output\")\n",
    "        txt_path = os.path.join(output_folder, txt_filename)\n",
    "        \n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, 'r') as txt_file:\n",
    "                label = txt_file.read().strip()\n",
    "\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "        print(f\"performing task on {image_path}\")\n",
    "        image = Image.open(image_path)\n",
    "        image_simpler = convert_image_to_white_or_black_r_image(image)\n",
    "\n",
    "        image_list = crop_image_and_return_5char(image, coordinates)\n",
    "        image_simpler_list = crop_image_and_return_5char(image_simpler, coordinates)\n",
    "        for i, char_image in enumerate(image_list):\n",
    "            char_label = label[i]\n",
    "            if len(filter_for_characters) == 0 or (char_label in filter_for_characters):\n",
    "                char_filename = f\"{os.path.splitext(filename)[0]}_char{i+1}.png\"\n",
    "                char_label_filename = f\"{os.path.splitext(txt_filename)[0]}_char{i+1}.txt\"\n",
    "                char_image.save(os.path.join(letters_input_folder, char_filename))\n",
    "                char_image_simpler = image_simpler_list[i]\n",
    "                char_simpler_filename = f\"{os.path.splitext(filename)[0]}_char{i+1}_worb.png\"\n",
    "                char_image_simpler.save(os.path.join(letters_input_folder, char_simpler_filename))\n",
    "                \n",
    "                with open(os.path.join(letters_output_folder, char_label_filename), 'w') as char_txt_file:\n",
    "                    char_txt_file.write(char_label)\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0306619-c0a2-4b0c-b5de-13e62e1bcc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD//gA+Q1JFQVRPUjogZ2QtanBlZyB2MS4wICh1c2luZyBJSkcgSlBFRyB2NjIpLCBkZWZhdWx0IHF1YWxpdHkK/9sAQwAIBgYHBgUIBwcHCQkICgwUDQwLCwwZEhMPFB0aHx4dGhwcICQuJyAiLCMcHCg3KSwwMTQ0NB8nOT04MjwuMzQy/9sAQwEJCQkMCwwYDQ0YMiEcITIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIy/8AAEQgAHgA8AwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/aAAwDAQACEQMRAD8A9xma8hQN50By6r/qT3YD+971G9zNFcNFLd2seEVgXjIzkkf3/b9a5r4j+JLvw3pukPbT2dv9t1a3s5ri7UskEbbmLnDL02A8kDGfqE8N6nd6pfaiv9uWl1qEEHlFH0ma0e1YgMplikk3sp3Aj7ucNg+gBvTXgPlo11byqWDnygo+6wPUvgZ/xqRdVLQeaq2pOwv5YuPmwBnGNvWuW8IeJ9d1Qa9NqRsrhNM1G506O3sbN0lnkiCkMC8xUbhkbTgA4+bFY2kePtR1zVtJtJ0srs3Uk/2zS7GGZbvSwuVBmZzghc7XyqEsRsD9CAekW1pbSxu8lvE7GWTLMgJPzmknitbSa2lEcMI80guFC8bG715mvxG1qD4frdx21k3idtVbSmsDBL5YuzMf3e7fgnZzndjPftU+l+ONR1rVdTtpdf8ADmn3NjrE1ja2U9ozT3AB2KwVrhDzuIwB1B+lAHoEdzE6sz6mEO9wFDR4ADEDqPTFPt7sND8zySEMw3rCzBgGIByox0x0pm6VLG3gktpFKNCpYlSMhl9Dn9K1qAOY1fT9TvLBpNBaysL2CffFJLGrxylGIKSDZkI2MEqQw4IPGDh6R4f1i38X3fiC8j01dQXTzp1taW0r+UY1KybpJTHkuzNjhMADoxrurP8A1Lf9dZP/AENqzZbyK01GSRlcglh8oGfuxf4UAch4a8N65p9tr0GpiC3t9X1K8vPtGmXcrz27Srs2qPJH3cEh89cfLVfSfCOpKPDFjdfYLWx8OyvJFeabDMtzdEZUKVKARK4JMoDPvPpnI9F0+QSWm9QQGkc89eXNVYrdbpTN5dr88jqN9vuOQxGSd3PSgDjbT4du3xH/AOEie4f+xi73w01nIX7fkoJfKK7fuYbdndv5zim6Hpnijw7LrqW+l6Zd2uo67NfxzSai8RUPIoClRC3Py9c9/au3itYvtMSSQWrKyM/ywBSCCvufWrnkw+X5HlJ5f9zaNvr0oAS8/wBSv/XWP/0Nas1mX0NraW/nrbxqY3Q5RAD94dKj/wCEgtP+ec/5D/GgD//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=60x30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "result after conversion\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAeADwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iuP+Ifim88Kado89nLYw/btVhsZp75C0cEbhy0hAdPu7QeSBjP1qx4c1q61mLVIRrNjPfW22Py/7IntHtnZSymWKWUuykFSMbc4bB9ADqKK4fwX4j8Qa3/wkM2o/YZ49J1C505LextWikneLaQwaSYqN2SNpxzj5sVl+HfiHquq6zpFtImm3b37zrdaXZRyLd6SEbAadpDghfutlYyWI2B+hAPTKK8v/wCFj6x/wrj7b9msf+Eu/tX+xvsGx/J+2edjy87sf6v5t2/bnv2qTR/HWpazrOs2D+I/DGlT2msT6da2d1bM886qwCMB9oQksTjheSDj0oA9MooooAy9ftdXutLYaHqEdlqEbiWIzRB4pcf8s5BjIRuhK4YcEHjBw9P8PawfFt/4t1JLFNS/s8afZWNtcO0IjDeZmSVowSxfjKphV7MenYUUAcH4S8K69ptv4mstUNpbQa1e3V8tzpt85nt2mCjau6JRlQCQ+eoHy1T0vwFrEf8Awi9heHSrSx8NytLDeaaXW5uscKpUqBErgkygM+8+mcj0iigDz/8A4Vx/xdD/AISH7T/xJP8Aj+/s3f8Au/7Rxs87ytu37nzbs79/OcVJ4d0PxV4XuNfW3stGvYNS1i41GN5NRlhZFkIwpUQMMgL6967yigAooooA/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADwAAAAeCAIAAAD/+uoYAAABJklEQVR4Ae1VQQ7DMAhbpv3/y50zVAtBoDmgbZXoYSLGgHGjbhzH8bjb87yb4Km3RX/rrbXT7XTiQF+PxJzSVDtdamfS7GVyYwyD4Mi/ep9lCjSdJS4gj6QB0XwO1UyCJrCiJR1VCu51oEqDiPFETYyCTZquqrnTkIimHK9308Oq4lD0x6z5szPJqNQ7JKmdzkvO+npgKocxXtYTBB9MPIJIObNJwBJwNqusaFNGKQb3ImS20Kb2c23PNMhlZ8PHMbwenpogkIgsx1O6lPBoaEnDPGVFT4vOV5xX/jBrRdMV0SQL0MJI6GUVOzCIWu3g66+pNluP0bh0j7IaF6bf33cD0xf6NdaiPe+vEHs9/kpcJKZFR85U4+10taNRv3Y6cqYav6XTb2G7jTyS9L7TAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=60x30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample white or black conversion demonstration\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "i = Image.open(r'.\\torch_training_data\\input\\input19.jpg')\n",
    "\n",
    "def convert_image_to_white_or_black(image):\n",
    "    \"\"\"\n",
    "    This function convert a PIL image to a black (0 value) or white (255 value) image\n",
    "    args:\n",
    "        img : PIL image\n",
    "    output:\n",
    "        result img: PIL image\n",
    "    \"\"\"\n",
    "    image_np = np.array(image)\n",
    "    image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).float()\n",
    "    binary_tensor = (image_tensor >= 128).to(torch.uint8)\n",
    "    result_tensor = binary_tensor * 255\n",
    "    result_np = result_tensor.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "    result_image = Image.fromarray(result_np)\n",
    "    return result_image\n",
    "r = convert_image_to_white_or_black(i)\n",
    "print(\"original\")\n",
    "display(i)\n",
    "print()\n",
    "print(\"result after conversion\")\n",
    "display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef720c2-d4e9-4342-b40f-0f4427e733f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
